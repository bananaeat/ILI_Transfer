{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on US region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/England/queries_filtered.txt') as f:\n",
    "    raw_us_queries = f.read().splitlines()\n",
    "query_ids_list = [int(r.split('\\t')[0]) for r in raw_us_queries]\n",
    "query_ids = set(query_ids_list)\n",
    "q_freq_us_raw = pd.read_csv('./data/US/Q_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids_dict = {int(q): k for q, k in [query.split('\\t') for query in raw_us_queries]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_freq_raw_filtered_us = q_freq_us_raw[q_freq_us_raw['Query'].isin(query_ids)]\n",
    "q_freq_raw_filtered_us = q_freq_raw_filtered_us.pivot(index='Day', columns='Query', values='Frequency')\n",
    "q_freq_raw_filtered_us = q_freq_raw_filtered_us.reindex(range(q_freq_raw_filtered_us.index.min(), q_freq_raw_filtered_us.index.max() + 1), fill_value=None)\n",
    "q_freq_raw_filtered_us = q_freq_raw_filtered_us.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def days_between(d1, d2, time_format=\"%Y-%m-%d\"):\n",
    "    d1 = datetime.strptime(d1, time_format)\n",
    "    d2 = datetime.strptime(d2, time_format)\n",
    "    return (d2 - d1).days\n",
    "\n",
    "def day_index(d, time_format=\"%Y-%m-%d\"):\n",
    "    start_date = '2004-01-01'\n",
    "    # Reformat the start date to match the time format\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\").strftime(time_format)\n",
    "    return days_between(start_date, d, time_format=time_format)\n",
    "\n",
    "def process_query_data(q_freq_raw, query_ids=query_ids):\n",
    "    q_freq_raw_filtered  = q_freq_raw[q_freq_raw['Query'].isin(query_ids)]\n",
    "    q_freq_raw_filtered = q_freq_raw_filtered.pivot(index='Day', columns='Query', values='Frequency')\n",
    "\n",
    "    # Fill in missing days with NaN\n",
    "    q_freq_raw_filtered = q_freq_raw_filtered.reindex(range(q_freq_raw_filtered.index.min(), q_freq_raw_filtered.index.max() + 1), fill_value=None)\n",
    "    # Replace missing values with zeroes\n",
    "    q_freq_raw_filtered = q_freq_raw_filtered.fillna(0)\n",
    "\n",
    "    # Apply harmonic mean to the data: for each day, use the harmonic weighted mean of the frequencies back to T-6 (or the start of the data if there are fewer than 14 days of data before T) to smooth the data\n",
    "    # harmonic_length = 7\n",
    "    # harmonic_filter = np.array([(harmonic_length-i)/harmonic_length for i in range(harmonic_length+1)])\n",
    "    # harmonic_filter = np.array([1/i for i in range(1, harmonic_length+1)])\n",
    "    # harmonic_weight_factor = [1 / sum(harmonic_filter[:i]) for i in range(1, harmonic_length+1)]\n",
    "\n",
    "    harmonic_length = 14\n",
    "    harmonic_filter = np.array([1/(i+1) for i in range(harmonic_length+1)])\n",
    "    # harmonic_filter = np.array([1/i for i in range(1, harmonic_length+1)])\n",
    "    harmonic_weight_factor = [1 / sum(harmonic_filter[:i]) for i in range(1, harmonic_length+1)]\n",
    "\n",
    "    q_freq_raw_filtered_smoothed = deepcopy(q_freq_raw_filtered)\n",
    "    for i in range(1, q_freq_raw_filtered.shape[0]):\n",
    "        harmonic_range = min(i + 1, harmonic_length)\n",
    "        harmonic_values = q_freq_raw_filtered.iloc[i - harmonic_range + 1:i + 1].values\n",
    "        harmonic_weights = harmonic_filter[:harmonic_range][::-1]\n",
    "        weighted_sum = (harmonic_values * harmonic_weights[:, None]).sum(axis=0)\n",
    "        q_freq_raw_filtered_smoothed.iloc[i] = weighted_sum * harmonic_weight_factor[harmonic_range - 1]\n",
    "\n",
    "    # Min-max normalize the data, grouping by query\n",
    "    query_min = q_freq_raw_filtered_smoothed.min()\n",
    "    query_max = q_freq_raw_filtered_smoothed.max()\n",
    "    q_freq_raw_filtered_smoothed = (q_freq_raw_filtered_smoothed - query_min) / (query_max - query_min)\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    q_freq_raw_filtered_smoothed = q_freq_raw_filtered_smoothed.fillna(0)\n",
    "\n",
    "    return q_freq_raw_filtered_smoothed, query_min, query_max\n",
    "\n",
    "q_freq_us_smoothed, query_min_us, query_max_us = process_query_data(q_freq_us_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data_for_season(start_date, end_date, q_freq_raw_filtered):\n",
    "    start_day = day_index(start_date)\n",
    "    end_day = day_index(end_date)\n",
    "    q_freq_raw_filtered_season = q_freq_raw_filtered.iloc[start_day:end_day+1]\n",
    "    return q_freq_raw_filtered_season\n",
    "\n",
    "all_seasons = [('2008-09-01', '2009-08-31'), ('2009-09-01', '2010-08-31'), ('2010-09-01', '2011-08-31'), ('2011-09-01', '2012-08-31'), ('2012-09-01', '2013-08-31'), ('2013-09-01', '2014-08-31'), ('2014-09-01', '2015-08-31'), ('2015-09-01', '2016-08-31'), ('2016-09-01', '2017-08-31'), ('2017-09-01', '2018-08-31'), ('2018-09-01', '2019-08-31')]\n",
    "\n",
    "test_seasons = all_seasons[-2:]\n",
    "train_seasons = all_seasons[:-2]\n",
    "training_query_data = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons]\n",
    "testing_query_data = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili_raw_us = pd.read_csv('./data/US/ILINet.csv', skiprows=1)\n",
    "\n",
    "ili_raw_us = ili_raw_us[['YEAR', 'WEEK', 'ILITOTAL']]\n",
    "\n",
    "# Turn year and week number into date\n",
    "def year_and_week_to_date(year, week):\n",
    "    # For example, year 2008 and week 35 (end of week) is 2008\n",
    "\n",
    "    # Find the first day of the year\n",
    "    first_day = datetime.strptime(f'{year}-01-01', \"%Y-%m-%d\")\n",
    "    # Find the Wednesday of the week\n",
    "    first_day_of_week = first_day + pd.DateOffset(weeks=week-1, days=2-first_day.weekday())\n",
    "    \n",
    "    # Return a string\n",
    "    return first_day_of_week.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "ili_raw_us['WeekStart'] = ili_raw_us.apply(lambda x: year_and_week_to_date(x['YEAR'], x['WEEK']), axis=1)\n",
    "ili_raw_us\n",
    "\n",
    "# From 2008 to 2019\n",
    "us_population_dict = {2008: 305694910, 2009: 308512035, 2010: 311182845, 2011: 313876608, 2012: 316651321, 2013: 319375166, 2014: 322033964, 2015: 324607776, 2016: 327210198, 2017: 329791231, 2018: 332140037, 2019: 334319671}\n",
    "\n",
    "ili_raw_us['Population'] = np.nan\n",
    "for year, population in us_population_dict.items():\n",
    "    first_date_index = ili_raw_us[ili_raw_us['YEAR'] == year].index[0]\n",
    "    ili_raw_us.at[first_date_index, 'Population'] = population\n",
    "\n",
    "# Linear interpolation for missing values\n",
    "ili_raw_us['ILITOTAL'] = ili_raw_us['ILITOTAL'].interpolate(method='linear')\n",
    "ili_raw_us['Population'] = ili_raw_us['Population'].interpolate(method='linear')\n",
    "\n",
    "# Calculate ILI rate\n",
    "ili_raw_us['ILIRate'] = ili_raw_us['ILITOTAL'] / ili_raw_us['Population'] * 100000\n",
    "\n",
    "dataset_start_day = day_index('2008-09-01')\n",
    "dataset_end_day = day_index('2019-08-31')\n",
    "\n",
    "def preprocess_ili_data(ili_raw, time_format=\"%Y-%m-%d\", training_end_date='2017-08-31', training_start_date='2008-09-01'):\n",
    "    ili_raw['Day'] = ili_raw['WeekStart'].apply(day_index, time_format=time_format)\n",
    "    ili_raw = ili_raw.set_index('Day')\n",
    "    ili_raw = ili_raw[~ili_raw.index.duplicated(keep='first')]\n",
    "    ili_raw = ili_raw.reindex(range(ili_raw.index.min(), ili_raw.index.max() + 1), fill_value=None)\n",
    "    ili_raw = ili_raw.reset_index()\n",
    "\n",
    "    ili_raw = ili_raw[['Day', 'ILIRate']]\n",
    "    ili_raw = ili_raw[ili_raw['Day'].between(dataset_start_day, dataset_end_day)]\n",
    "\n",
    "    # Interpolate missing values with linear interpolation\n",
    "    ili_raw['ILIRate'] = ili_raw['ILIRate'].interpolate(method='linear')\n",
    "\n",
    "    # Extrapolate so starting days and ending days have values\n",
    "    ili_raw['ILIRate'] = ili_raw['ILIRate'].bfill()\n",
    "\n",
    "    ili_raw_training = ili_raw[ili_raw['Day'].between(day_index(training_start_date), day_index(training_end_date))]\n",
    "    # Find max and min for training data\n",
    "    max_ili = ili_raw_training['ILIRate'].max()\n",
    "    min_ili = ili_raw_training['ILIRate'].min()\n",
    "\n",
    "    # Normalize ILI rates\n",
    "    ili_raw['ILIRate'] = (ili_raw['ILIRate'] - min_ili) / (max_ili - min_ili)\n",
    "    \n",
    "    return ili_raw, min_ili, max_ili\n",
    "\n",
    "def ili_data_for_season(start_date, end_date, ili_raw):\n",
    "    start_day = day_index(start_date)\n",
    "    end_day = day_index(end_date)\n",
    "    ili_raw_season = ili_raw[ili_raw['Day'].between(start_day, end_day)]\n",
    "\n",
    "    # Set Day as index\n",
    "    ili_raw_season = ili_raw_season.set_index('Day')\n",
    "\n",
    "    return ili_raw_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ili_data, min_ili, max_ili = preprocess_ili_data(ili_raw_us)\n",
    "\n",
    "training_ili_data = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons]\n",
    "testing_ili_data = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random query in the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "query_id = 1\n",
    "unsmoothed_time_series = q_freq_raw_filtered_us[query_id][day_index('2015-09-01'):day_index('2015-10-31') + 1]\n",
    "average_smoothed_time_series = unsmoothed_time_series.rolling(window=7).mean()\n",
    "harmonic_smoothed_time_series = deepcopy(unsmoothed_time_series)\n",
    "\n",
    "harmonic_length = 7\n",
    "harmonic_filter = np.array([(harmonic_length-i)/harmonic_length for i in range(harmonic_length+1)])\n",
    "harmonic_weight_factor = [1 / sum(harmonic_filter[:i]) for i in range(1, harmonic_length+1)]\n",
    "\n",
    "for i in range(1, len(harmonic_smoothed_time_series)):\n",
    "    harmonic_range = min(i + 1, harmonic_length)\n",
    "    harmonic_values = unsmoothed_time_series[i - harmonic_range + 1:i + 1]\n",
    "    harmonic_weights = harmonic_filter[:harmonic_range][::-1]\n",
    "    weighted_sum = (harmonic_values * harmonic_weights).sum()\n",
    "    harmonic_smoothed_time_series[day_index('2015-09-01') + i] = weighted_sum * harmonic_weight_factor[harmonic_range - 1]\n",
    "\n",
    "# Limit x axis to 2015-09-01 to 2015-10-31\n",
    "plt.plot(unsmoothed_time_series, label=\"Unsmoothed\")\n",
    "plt.plot(average_smoothed_time_series, label=\"Average smoothed\")\n",
    "plt.plot(harmonic_smoothed_time_series, label=\"Adjusted smoothed\")\n",
    "\n",
    "plt.title(f\"Query 'flu medicine' between 2015-09-01 and 2015-10-31\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/query_smoothing.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ILI data by seasons, combined in one plot\n",
    "\n",
    "plt.plot(pd.concat(training_ili_data), label=\"Training\")\n",
    "plt.plot(pd.concat(testing_ili_data), label=\"Testing\")\n",
    "\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Normalized ILI Rate\")\n",
    "\n",
    "# Change the x-axis to show the dates of start of each season\n",
    "plt.xticks([day_index(start_date) for start_date, _ in train_seasons + test_seasons], [start_date for start_date, _ in train_seasons + test_seasons], rotation=60)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('ILI_Rate_by_Season.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(query_id, query_data, ili_data):\n",
    "    query_values = query_data[query_id].values\n",
    "    ili_values = ili_data['ILIRate'].values\n",
    "    return pd.Series(query_values).corr(pd.Series(ili_values))\n",
    "\n",
    "def filter_correlated_queries(query_data_seasons, ili_data_seasons, num_of_features=50):\n",
    "    qids_correlated = set()\n",
    "    qids_candidates = set(query_data_seasons[0].columns)\n",
    "    qid_correlation_map = {}\n",
    "    for qid in qids_candidates:\n",
    "        qid_not_in_all_data = False\n",
    "        qid_correlation = 0\n",
    "        # Check not all values are constant\n",
    "        # Only calculate correlation for the last 5 seasons\n",
    "        for i, query_data in enumerate(query_data_seasons[-5:]):\n",
    "            if qid not in query_data.columns:\n",
    "                qid_not_in_all_data = True\n",
    "                break\n",
    "            if query_data[qid].std() == 0:\n",
    "                qid_correlation += 0\n",
    "                continue\n",
    "            qid_correlation += pearson_correlation(qid, query_data, ili_data_seasons[i])\n",
    "\n",
    "        if qid_not_in_all_data:\n",
    "            continue\n",
    "        qid_correlation_map[qid] = qid_correlation\n",
    "\n",
    "    qid_correlation_map = {k: v for k, v in sorted(qid_correlation_map.items(), key=lambda item: item[1], reverse=True)}\n",
    "    qids_all_sorted = list(qid_correlation_map.keys())\n",
    "    qids_correlated = list(qid_correlation_map.keys())[:num_of_features]\n",
    "    \n",
    "    query_data_seasons_correlated = [query_data[qids_correlated] for query_data in query_data_seasons]\n",
    "    return query_data_seasons_correlated, qids_correlated, qids_all_sorted\n",
    "\n",
    "Qtrain_correlated, qids_correlated, qids_all_sorted = filter_correlated_queries(training_query_data, training_ili_data, num_of_features=50)\n",
    "Qtest_correlated = [query_data[list(qids_correlated)] for query_data in testing_query_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nowcast Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Fully Connected Neural Network. Input size is the number of queries * window size, output size is 1\n",
    "class NowcastFCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NowcastFCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation = nn.ReLU()\n",
    "        # self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        # self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        # out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation(out)\n",
    "        # out = self.bn2(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training and testing FCNN\n",
    "\n",
    "# Expand Y from day T to from T-output_window_size//2 to T+output_window_size//2 for each day T. output_window_size needs to be odd\n",
    "def extrapolate_y_windows(y_tensor, output_window_size):\n",
    "    expanded_y = []\n",
    "    half_window = output_window_size // 2\n",
    "    for i in range(len(y_tensor)):\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(len(y_tensor), i + half_window + 1)\n",
    "        window = y_tensor[start:end].flatten()\n",
    "        if len(window) < output_window_size:\n",
    "            if start == 0:  # extrapolate at the beginning\n",
    "                extrapolated_values = torch.linspace(window[0], window[0], half_window - i)\n",
    "                window = torch.cat((extrapolated_values, window))\n",
    "            if end == len(y_tensor):  # extrapolate at the end\n",
    "                extrapolated_values = torch.linspace(window[-1], window[-1], half_window - len(y_tensor) + i + 1)\n",
    "                window = torch.cat((window, extrapolated_values))\n",
    "        expanded_y.append(window)\n",
    "    return torch.stack(expanded_y)\n",
    "\n",
    "# Preprocess a split\n",
    "def preprocess_fold(X, Y, input_window_size=7, output_window_size=7):\n",
    "    X_windows = []\n",
    "    Y_windows = []\n",
    "    for fold in range(len(X)):\n",
    "        X_windows.append([])\n",
    "        Y_windows.append([])\n",
    "        for i in range(input_window_size, len(X[fold])):\n",
    "            X_windows[fold].append(X[fold][i-input_window_size:i].values.flatten())\n",
    "            Y_windows[fold].append(Y[fold].iloc[i]['ILIRate'])\n",
    "        X_windows[fold] = torch.tensor(X_windows[fold], dtype=torch.float32)\n",
    "        Y_windows[fold] = torch.tensor(Y_windows[fold], dtype=torch.float32)\n",
    "\n",
    "    # Combine training data into one tensor\n",
    "    X_windows = torch.cat(X_windows)\n",
    "    Y_windows = torch.cat(Y_windows).view(-1, 1)\n",
    "    Y_expanded = extrapolate_y_windows(Y_windows, output_window_size)\n",
    "\n",
    "    return X_windows, Y_expanded\n",
    "\n",
    "def validation_fold(X_train, Y_train):\n",
    "    # For validation, use 0-100 days from season -3, 101-200 days from season -2, 201-end from season -1, concatenate them\n",
    "    # Concatenate the data from the three seasons\n",
    "    X_val = [X_train[-3][:100], X_train[-2][100:200], X_train[-1][200:]]\n",
    "    Y_val = [Y_train[-3][:100], Y_train[-2][100:200], Y_train[-1][200:]]\n",
    "\n",
    "    # Remove the validation data (only these days, not the entire seasons) from training data\n",
    "    X_train[-3] = X_train[-3][100:]\n",
    "    X_train[-2] = pd.concat([X_train[-2][:100], X_train[-2][200:]])\n",
    "    X_train[-1] = X_train[-1][:200]\n",
    "    Y_train[-3] = Y_train[-3][100:]\n",
    "    Y_train[-2] = pd.concat([Y_train[-2][:100], Y_train[-2][200:]])\n",
    "    Y_train[-1] = Y_train[-1][:200]\n",
    "\n",
    "    return X_val, Y_val, X_train, Y_train\n",
    "\n",
    "def test_model(model, X_train_windows, Y_train_windows, X_test_windows, Y_test_windows, output_size=7, plot_graph=True, min_ili=None, max_ili=None):\n",
    "        # Test the model. Only pick the T day for testing\n",
    "        model.eval()\n",
    "        Y_pred = model(X_test_windows)\n",
    "        Y_pred = Y_pred[:, output_size // 2].view(-1, 1)\n",
    "        Y_test_windows = Y_test_windows[:, output_size // 2].view(-1, 1)\n",
    "\n",
    "        Y_pred_train = model(X_train_windows)\n",
    "        Y_pred_train = Y_pred_train[:, output_size // 2].view(-1, 1)\n",
    "        Y_train_windows = Y_train_windows[:, output_size // 2].view(-1, 1)\n",
    "\n",
    "        # Denormalize the data\n",
    "        if min_ili is not None and max_ili is not None:\n",
    "            Y_pred = Y_pred * (max_ili - min_ili) + min_ili\n",
    "            Y_test_windows = Y_test_windows * (max_ili - min_ili) + min_ili\n",
    "            Y_pred_train = Y_pred_train * (max_ili - min_ili) + min_ili\n",
    "            Y_train_windows = Y_train_windows * (max_ili - min_ili) + min_ili\n",
    "\n",
    "        # Plot the results\n",
    "        if plot_graph:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(Y_test_windows.cpu().numpy(), label='True')\n",
    "            plt.plot(Y_pred.cpu().detach().numpy(), label='Predicted')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Also test the model on the training data\n",
    "            plt.plot(Y_train_windows.cpu().numpy(), label='True')\n",
    "            plt.plot(Y_pred_train.cpu().detach().numpy(), label='Predicted')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        # MSE as evaluation\n",
    "        metric = nn.L1Loss()\n",
    "        train_loss = metric(Y_pred_train, Y_train_windows).item()\n",
    "        test_loss = metric(Y_pred, Y_test_windows).item()\n",
    "\n",
    "        # Forecast skill as evaluation\n",
    "        mse_metric = nn.MSELoss()\n",
    "        forecast_skill = 1 - mse_metric(Y_pred_train, Y_train_windows) / mse_metric(Y_pred, Y_test_windows)\n",
    "\n",
    "        # Pearson Correlation as evaluation\n",
    "        correlation = Y_pred.cpu().detach().numpy().flatten()\n",
    "        true_values = Y_test_windows.cpu().numpy().flatten()\n",
    "        correlation = pd.Series(correlation).corr(pd.Series(true_values))\n",
    "        return train_loss, test_loss, correlation, forecast_skill\n",
    "\n",
    "def prepare_test_data(model, X_test_windows, Y_test_windows, output_size=7, min_ili=None, max_ili=None):\n",
    "    Y_pred = model(X_test_windows)\n",
    "    Y_pred = Y_pred[:, output_size // 2].view(-1, 1)\n",
    "    Y_test_windows = Y_test_windows[:, output_size // 2].view(-1, 1)\n",
    "\n",
    "    # Denormalize the data\n",
    "    if min_ili is not None and max_ili is not None:\n",
    "        Y_pred = Y_pred * (max_ili - min_ili) + min_ili\n",
    "        Y_test_windows = Y_test_windows * (max_ili - min_ili) + min_ili\n",
    "\n",
    "    return Y_pred, Y_test_windows\n",
    "\n",
    "def prepare_test_data_scaled(model, X_test_windows, Y_test_windows, output_size=7, min_ili=None, max_ili=None, min_ili_scaled=None, max_ili_scaled=None):\n",
    "    Y_pred = model(X_test_windows)\n",
    "    Y_pred = Y_pred[:, output_size // 2].view(-1, 1)\n",
    "    Y_test_windows = Y_test_windows[:, output_size // 2].view(-1, 1)\n",
    "\n",
    "    # Denormalize the data\n",
    "    if min_ili is not None and max_ili is not None:\n",
    "        Y_pred = Y_pred * (max_ili - min_ili) + min_ili\n",
    "\n",
    "    if min_ili_scaled is not None and max_ili_scaled is not None:\n",
    "        Y_test_windows = Y_test_windows * (max_ili_scaled - min_ili_scaled) + min_ili_scaled\n",
    "\n",
    "    return Y_pred, Y_test_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(Qtrain, Qtest, ILITrain, ILITest, input_window_size=7, output_window_size=7, num_of_features=50, batch_size=28, random_seed=0):\n",
    "    # Determine the queries that are correlated with ILI rates based on training data, then filter the testing data based on the correlated queries\n",
    "    Qtrain_correlated, qids_correlated, _ = filter_correlated_queries(Qtrain, ILITrain, num_of_features=num_of_features)\n",
    "    Qtest_correlated = [query_data[list(qids_correlated)] for query_data in Qtest]\n",
    "\n",
    "    X_train = deepcopy(Qtrain_correlated)\n",
    "    Y_train = deepcopy(ILITrain)\n",
    "    X_test = Qtest_correlated\n",
    "    Y_test = ILITest\n",
    "\n",
    "    X_val, Y_val, X_train, Y_train = validation_fold(X_train, Y_train)\n",
    "\n",
    "    X_train, Y_train = preprocess_fold(X_train, Y_train, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "    X_val, Y_val = preprocess_fold(X_val, Y_val, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "    X_test, Y_test = preprocess_fold(X_test, Y_test, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "\n",
    "    # Move data to device\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    Y_test = Y_test.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    Y_val = Y_val.to(device)\n",
    "\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(random_seed)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "    return train_loader, X_train, Y_train, X_test, Y_test, X_val, Y_val\n",
    "\n",
    "def prepare_data_without_validation(Qtrain, Qtest, ILITrain, ILITest, input_window_size=7, output_window_size=7, num_of_features=50, batch_size=28):\n",
    "    # Determine the queries that are correlated with ILI rates based on training data, then filter the testing data based on the correlated queries\n",
    "    Qtrain_correlated, qids_correlated, _ = filter_correlated_queries(Qtrain, ILITrain, num_of_features=num_of_features)\n",
    "    Qtest_correlated = [query_data[list(qids_correlated)] for query_data in Qtest]\n",
    "\n",
    "    X_train = deepcopy(Qtrain_correlated)\n",
    "    Y_train = deepcopy(ILITrain)\n",
    "    X_test = Qtest_correlated\n",
    "    Y_test = ILITest\n",
    "\n",
    "    X_train, Y_train = preprocess_fold(X_train, Y_train, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "    X_test, Y_test = preprocess_fold(X_test, Y_test, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "\n",
    "    # Move data to device\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    Y_test = Y_test.to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(train_loader, X_train, Y_train, X_val, Y_val, hidden_size=128, learning_rate=0.01, max_epochs=200, input_window_size=7, output_window_size=7, plot_graph=False):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = output_window_size\n",
    "\n",
    "    print(\"Input size: \", input_size)\n",
    "\n",
    "    model = NowcastFCNN(input_size, hidden_size, output_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    best_val_loss = math.inf\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    warmup_epochs = 10\n",
    "    val_losses = []\n",
    "        \n",
    "    # Train the model\n",
    "    for i in tqdm(range(max_epochs)):\n",
    "        # Load data in batches\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            if(X_batch.shape[0] == 1):\n",
    "                continue\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_fn(Y_pred, Y_batch) + 1e-3 * sum(torch.linalg.norm(p, 1) for p in model.parameters())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        _, val_loss, _, _ = test_model(model, X_train, Y_train, X_val, Y_val, output_size=output_size, plot_graph=False)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = deepcopy(model)\n",
    "            patience_counter = 0\n",
    "        elif i > warmup_epochs:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience and i > warmup_epochs:\n",
    "            break\n",
    "\n",
    "    if plot_graph:\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()\n",
    "\n",
    "    return best_model, best_val_loss\n",
    "\n",
    "def plot_predictions(model, X_test, Y_test, start_date, end_date, output_size=7, min_ili=None, max_ili=None,  y_definition='ILI Rate', filename=\"nowcast.pdf\", savefig=False):\n",
    "    Y_pred, Y_test_post = prepare_test_data(model, X_test, Y_test, output_size=output_size, min_ili=min_ili, max_ili=max_ili)\n",
    "\n",
    "    # Find the day where difference is largest\n",
    "    diff = Y_pred - Y_test_post\n",
    "    max_diff = diff.abs().max()\n",
    "    max_diff_day = diff.abs().argmax().item()\n",
    "    max_diff_date = pd.Timedelta(days=max_diff_day) + pd.to_datetime(start_date)\n",
    "    print(f\"Max difference: {max_diff.item():.2f}% on {max_diff_date:%m/%d}\")\n",
    "\n",
    "    # Calculate the correlation between the predicted and true values\n",
    "    pred_values = Y_pred.cpu().detach().numpy().flatten()\n",
    "    true_values = Y_test_post.cpu().numpy().flatten()\n",
    "    correlation = pd.Series(pred_values).corr(pd.Series(true_values))\n",
    "    print(f\"Correlation: {correlation*100:.2f}%\")\n",
    "\n",
    "    # Calculate the mean absolute error\n",
    "    mae = nn.L1Loss()\n",
    "    mae_loss = mae(Y_pred, Y_test_post).item()\n",
    "    print(f\"MAE: {mae_loss:.3f}%\")\n",
    "\n",
    "    # Plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(Y_test_post.cpu().numpy(), label='True')\n",
    "    plt.plot(Y_pred.cpu().detach().numpy(), label='Predicted')\n",
    "\n",
    "    # Label the x-axis with the dates\n",
    "    days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    days = [day.strftime(\"%m-%d\") for day in days]\n",
    "    plt.xticks(range(0, len(days), 30), days[::30], rotation=60)\n",
    "    plt.ylim(0, 30)\n",
    "\n",
    "    # Label the y-axis with definition\n",
    "    plt.ylabel(y_definition)\n",
    "    plt.xlabel('Date')\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the figure as a pdf\n",
    "    if savefig:\n",
    "        plt.savefig(filename, format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_scaled(model, X_test, Y_test, start_date, end_date, output_size=7, min_ili=None, max_ili=None, min_ili_target=None, max_ili_target=None, y_definition='ILI Rate', max_query=None, verbal=True):\n",
    "    Y_pred, Y_test_post = prepare_test_data_scaled(model, X_test, Y_test, output_size=output_size, min_ili=min_ili, max_ili=max_ili, min_ili_scaled=min_ili_target, max_ili_scaled=max_ili_target)\n",
    "\n",
    "    if max_query is not None:\n",
    "        Y_pred = Y_pred * max_query\n",
    "\n",
    "    # Find the day where difference is largest\n",
    "    diff = Y_pred - Y_test_post\n",
    "    max_diff = diff.abs().max()\n",
    "    max_diff_day = diff.abs().argmax().item()\n",
    "    max_diff_date = pd.Timedelta(days=max_diff_day) + pd.to_datetime(start_date)\n",
    "    \n",
    "    # Calculate the correlation between the predicted and true values\n",
    "    pred_values = Y_pred.cpu().detach().numpy().flatten()\n",
    "    true_values = Y_test_post.cpu().numpy().flatten()\n",
    "    correlation = pd.Series(pred_values).corr(pd.Series(true_values))\n",
    "    \n",
    "    # Calculate the mean absolute error\n",
    "    mae = nn.L1Loss()\n",
    "    mae_loss = mae(Y_pred, Y_test_post).item()\n",
    "\n",
    "    if verbal:\n",
    "        print(f\"Max difference: {max_diff.item():.2f}% on {max_diff_date:%m/%d}\")\n",
    "        print(f\"Correlation: {correlation*100:.2f}%\")\n",
    "        print(f\"MAE: {mae_loss:.3f}%\")\n",
    "\n",
    "        # Plot the results\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(Y_test_post.cpu().numpy(), label='True')\n",
    "        plt.plot(Y_pred.cpu().detach().numpy(), label='Predicted')\n",
    "\n",
    "        # Fix Y scale\n",
    "        plt.ylim(0, 60)\n",
    "\n",
    "        # Label the x-axis with the dates\n",
    "        days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        days = [day.strftime(\"%m-%d\") for day in days]\n",
    "        plt.xticks(range(0, len(days), 30), days[::30], rotation=60)\n",
    "\n",
    "        # Label the y-axis with definition\n",
    "        plt.ylabel(y_definition)\n",
    "        plt.xlabel('Date')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return correlation, mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_with_hyperparameters(Qtrain, Qtest, ILITrain, ILITest, input_window_size=7, num_of_features=50, batch_size=28, hidden_size=128, plot_valid_graph=False, random_seed=0, output_window_size = 7):\n",
    "    torch.manual_seed(random_seed)\n",
    "    learning_rate = 0.001\n",
    "    max_epochs = 100\n",
    "    train_loader, X_train, Y_train, _, _, X_val, Y_val = prepare_data(Qtrain, Qtest, ILITrain, ILITest, input_window_size=input_window_size, output_window_size=output_window_size, num_of_features=num_of_features, batch_size=batch_size, random_seed=0)\n",
    "    model, val_loss = train_nn(train_loader, X_train, Y_train, X_val, Y_val, hidden_size=hidden_size, learning_rate=learning_rate, max_epochs=max_epochs, input_window_size=input_window_size, output_window_size=output_window_size, plot_graph=plot_valid_graph)\n",
    "    return model, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_random_seed = 5\n",
    "\n",
    "test_seasons_1 = all_seasons[-2:-1]\n",
    "train_seasons_1 = all_seasons[:-2]\n",
    "training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "processed_ili_data_us_1, min_ili_1, max_ili_1 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "testing_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "# Hyperparameters\n",
    "input_window_sizes = [7]\n",
    "nums_of_features = [60]\n",
    "hidden_sizes = [128]\n",
    "batch_sizes = [14]\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_hyperparameters_1 = None\n",
    "best_val_loss_1 = math.inf\n",
    "for input_window_size in input_window_sizes:\n",
    "    for num_of_features in nums_of_features:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(\"--------------------\")\n",
    "                print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                model, val_loss = train_nn_with_hyperparameters(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                print(\"Validation loss:\", val_loss)\n",
    "                if val_loss < best_val_loss_1:\n",
    "                    best_val_loss_1 = val_loss\n",
    "                    best_hyperparameters_1 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "# Train over entire training data with best hyperparameters\n",
    "input_window_size_1, num_of_features_1, hidden_size_1, batch_size_1 = best_hyperparameters_1\n",
    "print(\"Best hyperparameters:\", best_hyperparameters_1)\n",
    "model_us_1, val_loss_1 = train_nn_with_hyperparameters(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size_1, num_of_features=num_of_features_1, batch_size=batch_size_1, hidden_size=hidden_size_1, random_seed=universal_random_seed)\n",
    "\n",
    "train_loader_1, X_train_1, Y_train_1, X_test_1, Y_test_1 = prepare_data_without_validation(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size_1, output_window_size=7, num_of_features=num_of_features_1, batch_size=batch_size_1)\n",
    "\n",
    "test_seasons_2 = all_seasons[-1:]\n",
    "train_seasons_2 = all_seasons[:-1]\n",
    "training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "processed_ili_data_us_2, min_ili_2, max_ili_2 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "testing_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_hyperparameters_2 = None\n",
    "best_val_loss_2 = math.inf\n",
    "for input_window_size in input_window_sizes:\n",
    "    for num_of_features in nums_of_features:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(\"--------------------\")\n",
    "                print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                model, val_loss = train_nn_with_hyperparameters(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                print(\"Validation loss:\", val_loss)\n",
    "                if val_loss < best_val_loss_2:\n",
    "                    best_val_loss_2 = val_loss\n",
    "                    best_hyperparameters_2 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "# Train over entire training data with best hyperparameters\n",
    "input_window_size_2, num_of_features_2, hidden_size_2, batch_size_2 = best_hyperparameters_2\n",
    "print(\"Best hyperparameters:\", best_hyperparameters_2)\n",
    "model_us_2, val_loss_2 = train_nn_with_hyperparameters(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size_2, num_of_features=num_of_features_2, batch_size=batch_size_2, hidden_size=hidden_size_2, random_seed=universal_random_seed)\n",
    "\n",
    "train_loader_2, X_train_2, Y_train_2, X_test_2, Y_test_2 = prepare_data_without_validation(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size_2, output_window_size=7, num_of_features=num_of_features_2, batch_size=batch_size_2)\n",
    "\n",
    "plot_predictions(model_us_1, X_test_1, Y_test_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, y_definition='Number of ILI cases per 100,000 population (US)', savefig=True, filename=\"./plots/ffnn_us_1_regularized.pdf\")\n",
    "plot_predictions(model_us_2, X_test_2, Y_test_2, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, y_definition='Number of ILI cases per 100,000 population (US)', savefig=True, filename=\"./plots/ffnn_us_2_regularized.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load England data, baseline: Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_data_source_minmax(q_freq_raw, query_ids=query_ids, qmin=None, qmax=None):\n",
    "    q_freq_raw_filtered  = q_freq_raw[q_freq_raw['Query'].isin(query_ids)]\n",
    "    q_freq_raw_filtered = q_freq_raw_filtered.pivot(index='Day', columns='Query', values='Frequency')\n",
    "\n",
    "    # Fill in missing days with NaN\n",
    "    q_freq_raw_filtered = q_freq_raw_filtered.reindex(range(q_freq_raw_filtered.index.min(), q_freq_raw_filtered.index.max() + 1), fill_value=None)\n",
    "    # Replace missing values with zeroes\n",
    "    q_freq_raw_filtered = q_freq_raw_filtered.fillna(0)\n",
    "    q_freq_raw_filtered_smoothed = q_freq_raw_filtered\n",
    "\n",
    "    # Apply harmonic mean to the data: for each day, use the harmonic weighted mean of the frequencies back to T-13 (or the start of the data if there are fewer than 14 days of data before T) to smooth the data\n",
    "    harmonic_length = 7\n",
    "    harmonic_filter = np.array([(harmonic_length-i)/harmonic_length for i in range(harmonic_length+1)])\n",
    "    # harmonic_filter = np.array([1/i for i in range(1, harmonic_length+1)])\n",
    "    harmonic_weight_factor = [1 / sum(harmonic_filter[:i]) for i in range(1, harmonic_length+1)]\n",
    "\n",
    "    q_freq_raw_filtered_smoothed = deepcopy(q_freq_raw_filtered)\n",
    "    for i in range(1, q_freq_raw_filtered.shape[0]):\n",
    "        harmonic_range = min(i + 1, harmonic_length)\n",
    "        harmonic_values = q_freq_raw_filtered.iloc[i - harmonic_range + 1:i + 1].values\n",
    "        harmonic_weights = harmonic_filter[:harmonic_range][::-1]\n",
    "        weighted_sum = (harmonic_values * harmonic_weights[:, None]).sum(axis=0)\n",
    "        q_freq_raw_filtered_smoothed.iloc[i] = weighted_sum * harmonic_weight_factor[harmonic_range - 1]\n",
    "\n",
    "    # query_min_target = {}\n",
    "    # query_max_target = {}\n",
    "    # for query in q_freq_raw_filtered_smoothed.columns:\n",
    "    #     query_min_target[query] = q_freq_raw_filtered_smoothed[query].min()\n",
    "    #     query_max_target[query] = q_freq_raw_filtered_smoothed[query].max()\n",
    "    \n",
    "    # Normalize the data\n",
    "    # for query in q_freq_raw_filtered_smoothed.columns:\n",
    "    #     if query in qmin and query in qmax:\n",
    "    #         q_freq_raw_filtered_smoothed[query] = (q_freq_raw_filtered_smoothed[query] - qmin[query]) / (qmax[query] - qmin[query])\n",
    "    #     else:\n",
    "    #         q_freq_raw_filtered_smoothed[query] = (q_freq_raw_filtered_smoothed[query] - query_min_target[query]) / (query_max_target[query] - query_min_target[query])\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    q_freq_raw_filtered_smoothed = q_freq_raw_filtered_smoothed.fillna(0)\n",
    "    return q_freq_raw_filtered_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/England/queries_filtered.txt') as f:\n",
    "    raw_england_queries = f.read().splitlines()\n",
    "query_ids_list_england = [int(r.split('\\t')[0]) for r in raw_england_queries]\n",
    "query_ids_england = set(query_ids_list_england)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seasons = all_seasons[-2:]\n",
    "train_seasons = all_seasons[:-2]\n",
    "\n",
    "# Load England data\n",
    "q_freq_england_raw = pd.read_csv('./data/England/Q_freq.csv')\n",
    "q_freq_england_smoothed_normalized, qmin_england, qmax_england = process_query_data(q_freq_england_raw, query_ids=query_ids_england)\n",
    "q_freq_england_smoothed = process_query_data_source_minmax(q_freq_england_raw, query_ids=query_ids_england, qmin=query_min_us, qmax=query_max_us)\n",
    "ili_raw_england = pd.read_csv('./data/England/ILI_rates_RCGP.csv')\n",
    "processed_ili_data_england, min_ili_england, max_ili_england = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\")\n",
    "\n",
    "training_query_data_england = [query_data_for_season(start_date, end_date, q_freq_england_smoothed_normalized) for start_date, end_date in train_seasons]\n",
    "testing_query_data_england = [query_data_for_season(start_date, end_date, q_freq_england_smoothed_normalized) for start_date, end_date in test_seasons]\n",
    "training_ili_data_england = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons]\n",
    "testing_ili_data_england = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ILI data by seasons, combined in one plot\n",
    "\n",
    "plt.plot(pd.concat(training_ili_data_england), label=\"Training\")\n",
    "plt.plot(pd.concat(testing_ili_data_england), label=\"Testing\")\n",
    "\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Normalized ILI Rate\")\n",
    "\n",
    "# Change the x-axis to show the dates of start of each season\n",
    "plt.xticks([day_index(start_date) for start_date, _ in train_seasons + test_seasons], [start_date for start_date, _ in train_seasons + test_seasons], rotation=60)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('ILI_Rate_by_Season.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili_raw_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw ILI data in Us and England from 2010 to 2020\n",
    "\n",
    "ploting_us = ili_raw_us[ili_raw_us['Day'].between(day_index('2011-09-01'), day_index('2019-08-31'))]\n",
    "ploting_england = ili_raw_england[ili_raw_england['Day'].between(day_index('2011-09-01'), day_index('2019-08-31'))]\n",
    "\n",
    "plt.plot(ploting_us['Day'], ploting_us['ILIRate'], label=\"US\")\n",
    "plt.plot(ploting_england['Day'], ploting_england['ILIRate'], label=\"England\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"ILI Rate\")\n",
    "\n",
    "plt.xticks([day_index(str(year) + '-09-01') for year in range(2011, 2020)], range(2011, 2020), rotation=60)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('plots/ILI_Rate_US_England.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_freq_us_smoothed_englandids, _, _ = process_query_data(q_freq_us_raw, query_ids=query_ids_england)\n",
    "training_query_data_englandids = [query_data_for_season(start_date, end_date, q_freq_us_smoothed_englandids) for start_date, end_date in train_seasons]\n",
    "\n",
    "train_seasons = all_seasons[:-1]\n",
    "training_ili_data_us = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons]\n",
    "\n",
    "_, qids_correlated_englandids, qids_all_sorted_englandids = filter_correlated_queries(training_query_data_englandids, training_ili_data_us, num_of_features=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature mapping, non-uniform weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings of queries\n",
    "query_embeddings = torch.load('./data/US/queries_embeddings.pt')\n",
    "\n",
    "def query_similarity(qid1, qid2):\n",
    "    return F.cosine_similarity(query_embeddings[qid1], query_embeddings[qid2], dim=0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load embeddings\n",
    "query_embeddings = torch.load('./data/US/queries_embeddings.pt')\n",
    "\n",
    "def compute_cosine_similarity_matrix(embeddings):\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    similarity_matrix = torch.mm(embeddings, embeddings.t())\n",
    "    return similarity_matrix\n",
    "\n",
    "cosine_similarity_matrix = compute_cosine_similarity_matrix(query_embeddings)\n",
    "\n",
    "def query_mapping_baseline(source_query_ids, target_query_ids, k=1, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed):\n",
    "    # Does not map, just use the same queries\n",
    "    feature_dict = {}\n",
    "    target_query_ids = target_query_ids.intersection(target_query_data.columns)\n",
    "    target_query_ids = list(target_query_ids)\n",
    "\n",
    "    for source_query in source_query_ids:\n",
    "        if source_query in target_query_ids:\n",
    "            feature_dict[source_query] = [(source_query, 1)]\n",
    "        else:\n",
    "            feature_dict[source_query] = []\n",
    "    return feature_dict\n",
    "\n",
    "# Define the query mapping function\n",
    "def query_mapping(source_query_ids, target_query_ids, k=1, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=0.5, uniform_weight=False):\n",
    "    feature_dict = {}\n",
    "    \n",
    "    source_data = source_query_data[source_query_ids].values\n",
    "    target_query_ids = target_query_ids.intersection(target_query_data.columns)\n",
    "    target_query_ids = list(target_query_ids)\n",
    "    target_data = target_query_data[target_query_ids].values\n",
    "    # Truncate target or source data if the number of days is different\n",
    "    if source_data.shape[0] < target_data.shape[0]:\n",
    "        target_data = target_data[:source_data.shape[0]]\n",
    "    elif source_data.shape[0] > target_data.shape[0]:\n",
    "        source_data = source_data[:target_data.shape[0]]\n",
    "    correlation_matrix = np.corrcoef(source_data.T, target_data.T)[:len(source_query_ids), len(source_query_ids):]\n",
    "    combined_similarity_matrix = semantic_weight * cosine_similarity_matrix[source_query_ids][:, target_query_ids] + (1 - semantic_weight) * torch.tensor(correlation_matrix)\n",
    "    combined_similarity_matrix = combined_similarity_matrix.numpy()\n",
    "\n",
    "    for idx, source_query in enumerate(source_query_ids):\n",
    "        similarities = combined_similarity_matrix[idx]\n",
    "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "        top_k_similarities = similarities[top_k_indices]\n",
    "        \n",
    "        # Normalize the weights using softmax\n",
    "        if uniform_weight:\n",
    "            softmax_weights = np.ones(k) / k\n",
    "        else:\n",
    "            softmax_weights = np.exp(top_k_similarities) / np.sum(np.exp(top_k_similarities))\n",
    "        \n",
    "        # Map the source query to the top K target queries with weights\n",
    "        feature_dict[source_query] = [(target_query_ids[i], weight) for i, weight in zip(top_k_indices, softmax_weights)]\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Define the query mapping function\n",
    "def query_mapping_variable(source_query_ids, target_query_ids, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=0.5, uniform_weight=False, least_k=1):\n",
    "    feature_dict = {}\n",
    "    \n",
    "    source_data = source_query_data[source_query_ids].values\n",
    "    target_query_ids = target_query_ids.intersection(target_query_data.columns)\n",
    "    target_query_ids = list(target_query_ids)\n",
    "    target_data = target_query_data[target_query_ids].values\n",
    "    # Truncate target or source data if the number of days is different\n",
    "    if source_data.shape[0] < target_data.shape[0]:\n",
    "        target_data = target_data[:source_data.shape[0]]\n",
    "    elif source_data.shape[0] > target_data.shape[0]:\n",
    "        source_data = source_data[:target_data.shape[0]]\n",
    "    correlation_matrix = np.corrcoef(source_data.T, target_data.T)[:len(source_query_ids), len(source_query_ids):]\n",
    "    combined_similarity_matrix = semantic_weight * cosine_similarity_matrix[source_query_ids][:, target_query_ids] + (1 - semantic_weight) * torch.tensor(correlation_matrix)\n",
    "    combined_similarity_matrix = combined_similarity_matrix.numpy()\n",
    "\n",
    "    for idx, source_query in enumerate(source_query_ids):\n",
    "        similarities = combined_similarity_matrix[idx]\n",
    "        corr = 0\n",
    "        new_corr = 0\n",
    "        k = least_k\n",
    "        while new_corr >= corr and k <= 20:\n",
    "            corr = new_corr\n",
    "            top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "            top_k_similarities = similarities[top_k_indices]\n",
    "            \n",
    "            if uniform_weight:\n",
    "                softmax_weights = np.ones(k) / k\n",
    "            else:\n",
    "                softmax_weights = np.exp(top_k_similarities) / np.sum(np.exp(top_k_similarities))\n",
    "            \n",
    "            new_corr = np.corrcoef(source_data.T[idx], np.matmul(softmax_weights, target_data.T[top_k_indices]))[0, 1]\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        k = k - 1\n",
    "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "        top_k_similarities = similarities[top_k_indices]\n",
    "\n",
    "        if uniform_weight:\n",
    "            softmax_weights = np.ones(k) / k\n",
    "        else:\n",
    "            softmax_weights = np.exp(top_k_similarities) / np.sum(np.exp(top_k_similarities))\n",
    "        \n",
    "        # Map the source query to the top K target queries with weights\n",
    "        feature_dict[source_query] = [(target_query_ids[i], weight) for i, weight in zip(top_k_indices, softmax_weights)]\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model_us on England data with mapped queries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def prepare_data_with_mapping(source_Qtest, Qtest, ILITest, Qtrain, source_Qtrain, ILITrain, mapping, input_window_size=7, output_window_size=7, qids_correlated=qids_correlated, qmin_source=query_min_us, qmax_source=query_max_us):\n",
    "    # Determine the queries that are correlated with ILI rates based on training data, then filter the testing data based on the correlated queries\n",
    "    source_Qtest_correlated = [query_data[list(qids_correlated)] for query_data in source_Qtest]\n",
    "    source_Qtrain_correlated = [query_data[list(qids_correlated)] for query_data in source_Qtrain]\n",
    "\n",
    "    X_test = deepcopy(source_Qtest_correlated)\n",
    "    Y_test = deepcopy(ILITest)\n",
    "    X_train = deepcopy(source_Qtrain_correlated)\n",
    "    Y_train = deepcopy(ILITrain)\n",
    "\n",
    "    for original_season, mapped_season in zip(Qtest, X_test):\n",
    "        for source_query in mapped_season.columns:\n",
    "            mapped_season[source_query] = 0\n",
    "            for target_query, weight in mapping[source_query]:\n",
    "                mapped_season[source_query] += weight * original_season[target_query]\n",
    "\n",
    "    for original_season, mapped_season in zip(Qtrain, X_train):\n",
    "        for source_query in mapped_season.columns:\n",
    "            mapped_season[source_query] = 0\n",
    "            for target_query, weight in mapping[source_query]:\n",
    "                mapped_season[source_query] += weight * original_season[target_query]\n",
    "\n",
    "    # Baseline only: if the query is 0, set it to the average of all other queries\n",
    "    for season in X_test:\n",
    "        season_mean = season.mean(axis=1)\n",
    "        for query in season.columns:\n",
    "            if season[query].sum() == 0:\n",
    "                season[query] = season_mean\n",
    "\n",
    "    for season in X_train:\n",
    "        season_mean = season.mean(axis=1)\n",
    "        for query in season.columns:\n",
    "            if season[query].sum() == 0:\n",
    "                season[query] = season_mean\n",
    "\n",
    "    # For each query in qids_correlated, find min and max across X_train and X_test\n",
    "    qmin, qmax = {}, {}\n",
    "    for query in qids_correlated:\n",
    "        qmin[query] = X_train[-1][query].min()\n",
    "\n",
    "    qmax_all_season = {}\n",
    "    qmin_all_season = {}\n",
    "    for query in qids_correlated:\n",
    "        qmax_all_season[query] = X_train[-1][query].max()\n",
    "        qmin_all_season[query] = X_train[-1][query].min()\n",
    "\n",
    "    # Min-max normalize X_test based on source region min and max\n",
    "    for season in X_test:\n",
    "        for query in qids_correlated:\n",
    "            if qmax_all_season[query] - qmin_all_season[query] != 0:\n",
    "                season[query] = (season[query] - qmin_all_season[query]) / (qmax_all_season[query] - qmin_all_season[query])\n",
    "\n",
    "    harmonic_length = 7\n",
    "    harmonic_filter = np.array([(harmonic_length-i)/harmonic_length for i in range(harmonic_length+1)])\n",
    "    harmonic_weight_factor = [1 / sum(harmonic_filter[:i]) for i in range(1, harmonic_length+1)]\n",
    "    for k, season in enumerate(X_test):\n",
    "        season_smoothed = deepcopy(season)\n",
    "        for i in range(1, season.shape[0]):\n",
    "            harmonic_range = min(i + 1, harmonic_length)\n",
    "            harmonic_values = season.iloc[i - harmonic_range + 1:i + 1].values\n",
    "            harmonic_weights = harmonic_filter[:harmonic_range][::-1]\n",
    "            weighted_sum = (harmonic_values * harmonic_weights[:, None]).sum(axis=0)\n",
    "            season_smoothed.iloc[i] = weighted_sum * harmonic_weight_factor[harmonic_range - 1]\n",
    "        X_test[k] = season_smoothed\n",
    "        \n",
    "    qmax_all = 1\n",
    "\n",
    "    X_test_folds, Y_test_folds = preprocess_fold(X_test, Y_test, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "    X_train, Y_train = preprocess_fold(X_train, Y_train, input_window_size=input_window_size, output_window_size=output_window_size)\n",
    "\n",
    "    # Move data to device\n",
    "    X_test_folds = X_test_folds.to(device)\n",
    "    Y_test_folds = Y_test_folds.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "\n",
    "    return X_test_folds, Y_test_folds, X_train, Y_train, X_test, X_train, source_Qtest_correlated, qmax_all, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_transfer(train_loader, X_train, Y_train, X_val, Y_val, hidden_size=128, learning_rate=0.01, max_epochs=200, input_window_size=7, output_window_size=7, plot_graph=False):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = output_window_size\n",
    "\n",
    "    print(\"Input size: \", input_size)\n",
    "\n",
    "    model = NowcastFCNN(input_size, hidden_size, output_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    best_val_loss = math.inf\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    warmup_epochs = 5\n",
    "    val_losses = []\n",
    "\n",
    "    multiply_counter = 1\n",
    "        \n",
    "    # Train the model\n",
    "    for i in tqdm(range(max_epochs)):\n",
    "        # Load data in batches\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            if(X_batch.shape[0] == 1):\n",
    "                continue\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_fn(Y_pred, Y_batch) + 1e-3 * sum(torch.linalg.norm(p, 1) for p in model.parameters())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        _, val_loss, _, _ = test_model(model, X_train, Y_train, X_val, Y_val, output_size=output_size, plot_graph=False)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = deepcopy(model)\n",
    "            patience_counter = 0\n",
    "        elif i > warmup_epochs:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience and i > warmup_epochs:\n",
    "            break\n",
    "\n",
    "    if plot_graph:\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()\n",
    "\n",
    "    return best_model, best_val_loss\n",
    "\n",
    "def train_nn_with_hyperparameters_transfer(Qtrain, Qtest, ILITrain, ILITest, input_window_size=7, num_of_features=50, batch_size=28, hidden_size=128, plot_valid_graph=False, random_seed=0):\n",
    "    torch.manual_seed(random_seed)\n",
    "    output_window_size = 7\n",
    "    learning_rate = 0.001\n",
    "    max_epochs = 100\n",
    "    train_loader, X_train, Y_train, _, _, X_val, Y_val = prepare_data(Qtrain, Qtest, ILITrain, ILITest, input_window_size=input_window_size, output_window_size=output_window_size, num_of_features=num_of_features, batch_size=batch_size, random_seed=0)\n",
    "    model, val_loss = train_nn_transfer(train_loader, X_train, Y_train, X_val, Y_val, hidden_size=hidden_size, learning_rate=learning_rate, max_epochs=max_epochs, input_window_size=input_window_size, output_window_size=output_window_size, plot_graph=plot_valid_graph)\n",
    "    return model, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_random_seed = 5\n",
    "\n",
    "test_seasons_1 = all_seasons[-2:-1]\n",
    "train_seasons_1 = all_seasons[:-2]\n",
    "training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "processed_ili_data_us_1, min_ili_1, max_ili_1 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "testing_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "# Hyperparameters\n",
    "input_window_sizes = [7, 14]\n",
    "nums_of_features = [60, 100]\n",
    "hidden_sizes = [128, 256]\n",
    "batch_sizes = [14, 28]\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_hyperparameters_1 = None\n",
    "best_val_loss_1 = math.inf\n",
    "for input_window_size in input_window_sizes:\n",
    "    for num_of_features in nums_of_features:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(\"--------------------\")\n",
    "                print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                model, val_loss = train_nn_with_hyperparameters(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                print(\"Validation loss:\", val_loss)\n",
    "                if val_loss < best_val_loss_1:\n",
    "                    best_val_loss_1 = val_loss\n",
    "                    best_hyperparameters_1 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "# Train over entire training data with best hyperparameters\n",
    "input_window_size_1, num_of_features_1, hidden_size_1, batch_size_1 = best_hyperparameters_1\n",
    "print(\"Best hyperparameters:\", best_hyperparameters_1)\n",
    "model_us_1, val_loss_1 = train_nn_with_hyperparameters(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size_1, num_of_features=num_of_features_1, batch_size=batch_size_1, hidden_size=hidden_size_1, random_seed=universal_random_seed)\n",
    "\n",
    "train_loader_1, X_train_1, Y_train_1, X_test_1, Y_test_1 = prepare_data(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size_1, output_window_size=7, num_of_features=num_of_features_1, batch_size=batch_size_1)\n",
    "\n",
    "test_seasons_2 = all_seasons[-1:]\n",
    "train_seasons_2 = all_seasons[:-1]\n",
    "training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "processed_ili_data_us_2, min_ili_2, max_ili_2 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "testing_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_hyperparameters_2 = None\n",
    "best_val_loss_2 = math.inf\n",
    "for input_window_size in input_window_sizes:\n",
    "    for num_of_features in nums_of_features:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(\"--------------------\")\n",
    "                print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                model, val_loss = train_nn_with_hyperparameters(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                print(\"Validation loss:\", val_loss)\n",
    "                if val_loss < best_val_loss_2:\n",
    "                    best_val_loss_2 = val_loss\n",
    "                    best_hyperparameters_2 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "# Train over entire training data with best hyperparameters\n",
    "input_window_size_2, num_of_features_2, hidden_size_2, batch_size_2 = best_hyperparameters_2\n",
    "print(\"Best hyperparameters:\", best_hyperparameters_2)\n",
    "model_us_2, val_loss_2 = train_nn_with_hyperparameters(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size_2, num_of_features=num_of_features_2, batch_size=batch_size_2, hidden_size=hidden_size_2, random_seed=universal_random_seed)\n",
    "\n",
    "train_loader_2, X_train_2, Y_train_2, X_test_2, Y_test_2 = prepare_data(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size_2, output_window_size=7, num_of_features=num_of_features_2, batch_size=batch_size_2)\n",
    "\n",
    "plot_predictions(model_us_1, X_test_1, Y_test_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, y_definition='Number of ILI cases per 100,000 population (US)')\n",
    "plot_predictions(model_us_2, X_test_2, Y_test_2, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, y_definition='Number of ILI cases per 100,000 population (US)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two test seasons separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_random_seed = 50\n",
    "\n",
    "for universal_random_seed in range(start_seed, end_seed):\n",
    "    test_seasons_1 = all_seasons[-2:-1]\n",
    "    train_seasons_1 = all_seasons[:-2]\n",
    "    training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "    testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "    processed_ili_data_us_1, min_ili_1, max_ili_1 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "    training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "    testing_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_window_sizes = [7, 14, 21, 28]\n",
    "    nums_of_features = [60, 70, 80, 90, 100]\n",
    "    hidden_sizes = [128, 256, 512]\n",
    "    batch_sizes = [7, 14, 28, 56]\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    best_hyperparameters_1 = None\n",
    "    best_val_loss_1 = math.inf\n",
    "    for input_window_size in input_window_sizes:\n",
    "        for num_of_features in nums_of_features:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                for batch_size in batch_sizes:\n",
    "                    print(\"--------------------\")\n",
    "                    print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                    model, val_loss = train_nn_with_hyperparameters_transfer(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                    print(\"Validation loss:\", val_loss)\n",
    "                    if val_loss < best_val_loss_1:\n",
    "                        best_val_loss_1 = val_loss\n",
    "                        best_hyperparameters_1 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "    # Train over entire training data with best hyperparameters\n",
    "    input_window_size_1, num_of_features_1, hidden_size_1, batch_size_1 = best_hyperparameters_1\n",
    "    print(\"Best hyperparameters:\", best_hyperparameters_1)\n",
    "    model_us_1, val_loss_1 = train_nn_with_hyperparameters_transfer(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size_1, num_of_features=num_of_features_1, batch_size=batch_size_1, hidden_size=hidden_size_1, random_seed=universal_random_seed)\n",
    "\n",
    "    train_loader_1, X_train_1, Y_train_1, X_test_1, Y_test_1 = prepare_data_without_validation(training_query_data_us_1, testing_query_data_us_1, training_ili_data_us_1, testing_ili_data_us_1, input_window_size=input_window_size_1, output_window_size=7, num_of_features=num_of_features_1, batch_size=batch_size_1)\n",
    "\n",
    "    test_seasons_2 = all_seasons[-1:]\n",
    "    train_seasons_2 = all_seasons[:-1]\n",
    "    training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "    testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "    processed_ili_data_us_2, min_ili_2, max_ili_2 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "    training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "    testing_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    best_hyperparameters_2 = None\n",
    "    best_val_loss_2 = math.inf\n",
    "    for input_window_size in input_window_sizes:\n",
    "        for num_of_features in nums_of_features:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                for batch_size in batch_sizes:\n",
    "                    print(\"--------------------\")\n",
    "                    print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                    model, val_loss = train_nn_with_hyperparameters_transfer(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                    print(\"Validation loss:\", val_loss)\n",
    "                    if val_loss < best_val_loss_2:\n",
    "                        best_val_loss_2 = val_loss\n",
    "                        best_hyperparameters_2 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "    # Train over entire training data with best hyperparameters\n",
    "    input_window_size_2, num_of_features_2, hidden_size_2, batch_size_2 = best_hyperparameters_2\n",
    "    print(\"Best hyperparameters:\", best_hyperparameters_2)\n",
    "    model_us_2, val_loss_2 = train_nn_with_hyperparameters_transfer(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size_2, num_of_features=num_of_features_2, batch_size=batch_size_2, hidden_size=hidden_size_2, random_seed=universal_random_seed)\n",
    "\n",
    "    train_loader_2, X_train_2, Y_train_2, X_test_2, Y_test_2 = prepare_data_without_validation(training_query_data_us_2, testing_query_data_us_2, training_ili_data_us_2, testing_ili_data_us_2, input_window_size=input_window_size_2, output_window_size=7, num_of_features=num_of_features_2, batch_size=batch_size_2)\n",
    "\n",
    "    plot_predictions(model_us_1, X_test_1, Y_test_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, y_definition='Number of ILI cases per 100,000 population (US)')\n",
    "    plot_predictions(model_us_2, X_test_2, Y_test_2, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, y_definition='Number of ILI cases per 100,000 population (US)')\n",
    "\n",
    "    # Save the two models, along with the hyperparameters\n",
    "    torch.save(model_us_1, f'./models/model_us_1_{universal_random_seed}_with_cold.pt')\n",
    "    torch.save(model_us_2, f'./models/model_us_2_{universal_random_seed}_with_cold.pt')\n",
    "    with open(f'./models/hyperparameters_us_1_{universal_random_seed}_with_cold.txt', 'w') as f:\n",
    "        f.write(f\"{best_hyperparameters_1}\")\n",
    "    with open(f'./models/hyperparameters_us_2_{universal_random_seed}_with_cold.txt', 'w') as f:\n",
    "        f.write(f\"{best_hyperparameters_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seasons_1 = all_seasons[-2:-1]\n",
    "train_seasons_1 = all_seasons[:-2]\n",
    "test_seasons_2 = all_seasons[-1:]\n",
    "train_seasons_2 = all_seasons[:-1]\n",
    "training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "processed_ili_data_us_1, min_ili_1, max_ili_1 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "testing_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_1]\n",
    "training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "processed_ili_data_us_2, min_ili_2, max_ili_2 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "testing_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from local file\n",
    "def string_to_tuple(s):\n",
    "    return tuple(map(int, s.strip('()').split(', ')))\n",
    "\n",
    "def load_model_from_file(universal_random_seed):\n",
    "    model_us_1 = torch.load(f'./models/model_us_1_{universal_random_seed}_with_cold.pt')\n",
    "    model_us_2 = torch.load(f'./models/model_us_2_{universal_random_seed}_with_cold.pt')\n",
    "    with open(f'./models/hyperparameters_us_1_{universal_random_seed}_with_cold.txt', 'r') as f:\n",
    "        hyperparameters_1 = f.read()\n",
    "        hyperparameters_1 = string_to_tuple(hyperparameters_1)\n",
    "    with open(f'./models/hyperparameters_us_2_{universal_random_seed}_with_cold.txt', 'r') as f:\n",
    "        hyperparameters_2 = f.read()\n",
    "        hyperparameters_2 = string_to_tuple(hyperparameters_2)\n",
    "    return model_us_1, model_us_2, hyperparameters_1, hyperparameters_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test baseline method\n",
    "\n",
    "query_ids_subset = set(list(qids_all_sorted_englandids)[:500])\n",
    "\n",
    "corr_season1 = []\n",
    "corr_season2 = []\n",
    "mae_season1 = []\n",
    "mae_season2 = []\n",
    "\n",
    "train_seasons_1 = all_seasons[:-2]\n",
    "train_seasons_2 = all_seasons[:-1]\n",
    "test_seasons_1 = all_seasons[-2:-1]\n",
    "test_seasons_2 = all_seasons[-1:]\n",
    "\n",
    "processed_ili_data_us_1, min_ili_1, max_ili_1 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "processed_ili_data_us_2, min_ili_2, max_ili_2 = preprocess_ili_data(ili_raw_us, training_end_date='2018-08-31')\n",
    "\n",
    "# Load England data\n",
    "processed_ili_data_england_1, min_ili_england_1, max_ili_england_1 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2017-08-31', training_start_date='2008-09-01')\n",
    "training_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_1]\n",
    "training_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_1]\n",
    "testing_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_1]\n",
    "training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "processed_ili_data_england_2, min_ili_england_2, max_ili_england_2 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2018-08-31', training_start_date='2008-09-01')\n",
    "training_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_2]\n",
    "training_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_2]\n",
    "testing_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_2]\n",
    "training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "for seed in tqdm(range(start_seed, end_seed)):\n",
    "    model_us_1, model_us_2, hyperparameters_1, hyperparameters_2 = load_model_from_file(seed)\n",
    "    input_window_size_1, num_of_features_1, hidden_size_1, batch_size_1 = hyperparameters_1\n",
    "    input_window_size_2, num_of_features_2, hidden_size_2, batch_size_2 = hyperparameters_2\n",
    "\n",
    "    qids_correlated_us_1 = filter_correlated_queries(training_query_data_us_1, training_ili_data_us_1, num_of_features=num_of_features_1)[1]\n",
    "\n",
    "    mapping_us_to_england_1 = query_mapping_baseline(qids_correlated_us_1, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed)\n",
    "\n",
    "    X_test_mapped_1, Y_test_mapped_1, X_train_mapped_1, Y_train_mapped_1, X_test_1, X_train_1, source_X_test_1, qmax_1, Y_train_1, Y_test_1 = prepare_data_with_mapping(testing_query_data_us_1, testing_query_data_england_1, testing_ili_data_england_1, training_query_data_england_1, training_query_data_us_1, training_ili_data_us_1, mapping_us_to_england_1, input_window_size=input_window_size_1, output_window_size=7, qids_correlated=qids_correlated_us_1)\n",
    "\n",
    "    qids_correlated_us_2 = filter_correlated_queries(training_query_data_us_2, training_ili_data_us_2, num_of_features=num_of_features_2)[1]\n",
    "\n",
    "    mapping_us_to_england_2 = query_mapping_baseline(qids_correlated_us_2, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed)\n",
    "\n",
    "    X_test_mapped_2, Y_test_mapped_2, X_train_mapped_2, Y_train_mapped_2, X_test_2, X_train_2, source_X_test_2, qmax_2, Y_train_2, Y_test_2 = prepare_data_with_mapping(testing_query_data_us_2, testing_query_data_england_2, testing_ili_data_england_2, training_query_data_england_2, training_query_data_us_2, training_ili_data_us_2, mapping_us_to_england_2, input_window_size=input_window_size_2, output_window_size=7, qids_correlated=qids_correlated_us_2)\n",
    "\n",
    "    corr1, mae1 = plot_predictions_scaled(model_us_1, X_test_mapped_1, Y_test_mapped_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, min_ili_target=min_ili_england_1, max_ili_target=max_ili_england_1, y_definition='Number of ILI cases per 100,000 population (England)')\n",
    "    corr2, mae2 = plot_predictions_scaled(model_us_2, X_test_mapped_2, Y_test_mapped_2, '2018-09-01', '2019-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, min_ili_target=min_ili_england_2, max_ili_target=max_ili_england_2, y_definition='Number of ILI cases per 100,000 population (England)')\n",
    "\n",
    "    corr_season1.append(corr1)\n",
    "    corr_season2.append(corr2)\n",
    "    mae_season1.append(mae1)\n",
    "    mae_season2.append(mae2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(corr_season1),np.mean(corr_season2),np.mean(mae_season1),np.mean(mae_season2), (np.mean(corr_season1) + np.mean(corr_season2)) / 2, (np.mean(mae_season1) + np.mean(mae_season2)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_query_data_us_1[0].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids_subset = set(list(qids_all_sorted_englandids)[:500])\n",
    "k_value = 2\n",
    "semantic_weight = 0.2\n",
    "uniform_weight = False\n",
    "\n",
    "seed = 58\n",
    "\n",
    "corr_season1 = []\n",
    "corr_season2 = []\n",
    "mae_season1 = []\n",
    "mae_season2 = []\n",
    "\n",
    "model_us_1, model_us_2, hyperparameters_1, hyperparameters_2 = load_model_from_file(seed)\n",
    "input_window_size_1, num_of_features_1, hidden_size_1, batch_size_1 = hyperparameters_1\n",
    "input_window_size_2, num_of_features_2, hidden_size_2, batch_size_2 = hyperparameters_2\n",
    "\n",
    "train_seasons_1 = all_seasons[:-2]\n",
    "train_seasons_2 = all_seasons[:-1]\n",
    "test_seasons_1 = all_seasons[-2:-1]\n",
    "test_seasons_2 = all_seasons[-1:]\n",
    "\n",
    "processed_ili_data_us_1, min_ili_1, max_ili_1 = preprocess_ili_data(ili_raw_us, training_end_date='2017-08-31')\n",
    "processed_ili_data_us_2, min_ili_2, max_ili_2 = preprocess_ili_data(ili_raw_us, training_end_date='2018-08-31')\n",
    "\n",
    "# Load England data\n",
    "processed_ili_data_england_1, min_ili_england_1, max_ili_england_1 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2017-08-31', training_start_date='2008-09-01')\n",
    "training_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_1]\n",
    "training_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_1]\n",
    "testing_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_1]\n",
    "training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "qids_correlated_us_1 = filter_correlated_queries(training_query_data_us_1, training_ili_data_us_1, num_of_features=num_of_features_1)[1]\n",
    "\n",
    "# mapping_us_to_england_1 = query_mapping(qids_correlated_us_1, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight)\n",
    "mapping_us_to_england_1 = query_mapping(qids_correlated_us_1, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight)\n",
    "\n",
    "X_test_mapped_1, Y_test_mapped_1, X_train_mapped_1, Y_train_mapped_1, X_test_1, X_train_1, source_X_test_1, qmax_1, Y_train_1, Y_test_1 = prepare_data_with_mapping(testing_query_data_us_1, testing_query_data_england_1, testing_ili_data_england_1, training_query_data_england_1, training_query_data_us_1, training_ili_data_us_1, mapping_us_to_england_1, input_window_size=input_window_size_1, output_window_size=7, qids_correlated=qids_correlated_us_1)\n",
    "\n",
    "# Load England data\n",
    "processed_ili_data_england_2, min_ili_england_2, max_ili_england_2 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2018-08-31', training_start_date='2008-09-01')\n",
    "training_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_2]\n",
    "training_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_2]\n",
    "testing_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_2]\n",
    "training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "qids_correlated_us_2 = filter_correlated_queries(training_query_data_us_2, training_ili_data_us_2, num_of_features=num_of_features_2)[1]\n",
    "\n",
    "mapping_us_to_england_2 = query_mapping(qids_correlated_us_2, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight)\n",
    "\n",
    "X_test_mapped_2, Y_test_mapped_2, X_train_mapped_2, Y_train_mapped_2, X_test_2, X_train_2, source_X_test_2, qmax_2, Y_train_2, Y_test_2 = prepare_data_with_mapping(testing_query_data_us_2, testing_query_data_england_2, testing_ili_data_england_2, training_query_data_england_2, training_query_data_us_2, training_ili_data_us_2, mapping_us_to_england_2, input_window_size=input_window_size_2, output_window_size=7, qids_correlated=qids_correlated_us_2)\n",
    "\n",
    "corr1, mae1 = plot_predictions_scaled(model_us_1, X_test_mapped_1, Y_test_mapped_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, min_ili_target=min_ili_england_1, max_ili_target=max_ili_england_1, y_definition='Number of ILI cases per 100,000 population (England)')\n",
    "corr2, mae2 = plot_predictions_scaled(model_us_2, X_test_mapped_2, Y_test_mapped_2, '2018-09-01', '2019-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, min_ili_target=min_ili_england_2, max_ili_target=max_ili_england_2, y_definition='Number of ILI cases per 100,000 population (England)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average query data after query mapping, X_test_mapped_1, X_test_mapped_2\n",
    "\n",
    "plt.plot(X_train_mapped_1.cpu().mean(axis=-1))\n",
    "\n",
    "# Plot a fitting linear regression line\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.arange(len(X_train_mapped_1))\n",
    "reg = LinearRegression().fit(X.reshape(-1, 1), X_train_mapped_1.cpu().mean(axis=-1).numpy())\n",
    "plt.plot(X, reg.predict(X.reshape(-1, 1)), color='red', linewidth=2)\n",
    "\n",
    "# X-axis to start of each season, (use the function day_index)\n",
    "plt.xticks([day_index(str(year) + '-09-01') - day_index('2008-09-01') for year in range(2008, 2017)], range(2008, 2017), rotation=60)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of query frequencies, unnormalized')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('mean_query_freq.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transfer_parameter(k_value=5, semantic_weight=0.2, uniform_weight=False, candidate=500):\n",
    "    query_ids_subset = set(list(qids_all_sorted_englandids)[:candidate])\n",
    "\n",
    "    corr_season1 = []\n",
    "    corr_season2 = []\n",
    "    mae_season1 = []\n",
    "    mae_season2 = []\n",
    "\n",
    "    # Load England data\n",
    "    _, min_ili_england_1, max_ili_england_1 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2017-08-31', training_start_date='2008-09-01')\n",
    "    training_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_1]\n",
    "    testing_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_1]\n",
    "    testing_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_1]\n",
    "    training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "    training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "    testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "    _, min_ili_england_2, max_ili_england_2 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2018-08-31', training_start_date='2008-09-01')\n",
    "    training_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_2]\n",
    "    testing_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_2]\n",
    "    testing_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_2]\n",
    "    training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "    training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "    testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "    for seed in tqdm(range(start_seed, end_seed)):\n",
    "        model_us_1, model_us_2, hyperparameters_1, hyperparameters_2 = load_model_from_file(seed)\n",
    "        input_window_size_1, num_of_features_1, _, _ = hyperparameters_1\n",
    "        input_window_size_2, num_of_features_2, _, _ = hyperparameters_2\n",
    "\n",
    "        qids_correlated_us_1 = filter_correlated_queries(training_query_data_us_1, training_ili_data_us_1, num_of_features=num_of_features_1)[1]\n",
    "\n",
    "        mapping_us_to_england_1 = query_mapping(qids_correlated_us_1, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight)\n",
    "\n",
    "        X_test_mapped_1, Y_test_mapped_1, _, _, _, _, _, _, _, _ = prepare_data_with_mapping(testing_query_data_us_1, testing_query_data_england_1, testing_ili_data_england_1, training_query_data_england_1, training_query_data_us_1, training_ili_data_us_1, mapping_us_to_england_1, input_window_size=input_window_size_1, output_window_size=7, qids_correlated=qids_correlated_us_1)\n",
    "\n",
    "        qids_correlated_us_2 = filter_correlated_queries(training_query_data_us_2, training_ili_data_us_2, num_of_features=num_of_features_2)[1]\n",
    "\n",
    "        mapping_us_to_england_2 = query_mapping(qids_correlated_us_2, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight)\n",
    "\n",
    "        X_test_mapped_2, Y_test_mapped_2, _, _, _, _, _, _, _, _ = prepare_data_with_mapping(testing_query_data_us_2, testing_query_data_england_2, testing_ili_data_england_2, training_query_data_england_2, training_query_data_us_2, training_ili_data_us_2, mapping_us_to_england_2, input_window_size=input_window_size_2, output_window_size=7, qids_correlated=qids_correlated_us_2)\n",
    "\n",
    "        corr1, mae1 = plot_predictions_scaled(model_us_1, X_test_mapped_1, Y_test_mapped_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, min_ili_target=min_ili_england_1, max_ili_target=max_ili_england_1, y_definition='Number of ILI cases per 100,000 population (England)', verbal=False)\n",
    "        corr2, mae2 = plot_predictions_scaled(model_us_2, X_test_mapped_2, Y_test_mapped_2, '2018-09-01', '2019-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, min_ili_target=min_ili_england_2, max_ili_target=max_ili_england_2, y_definition='Number of ILI cases per 100,000 population (England)', verbal=False)\n",
    "\n",
    "        corr_season1.append(corr1)\n",
    "        corr_season2.append(corr2)\n",
    "        mae_season1.append(mae1)\n",
    "        mae_season2.append(mae2)\n",
    "\n",
    "    return corr_season1, corr_season2, mae_season1, mae_season2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transfer_parameter_variable(semantic_weight=0.2, uniform_weight=False, candidate=500, least_k=1):\n",
    "    query_ids_subset = set(list(qids_all_sorted_englandids)[:candidate])\n",
    "\n",
    "    corr_season1 = []\n",
    "    corr_season2 = []\n",
    "    mae_season1 = []\n",
    "    mae_season2 = []\n",
    "\n",
    "    for seed in tqdm(range(start_seed, end_seed)):\n",
    "        model_us_1, model_us_2, hyperparameters_1, hyperparameters_2 = load_model_from_file(seed)\n",
    "        input_window_size_1, num_of_features_1, hidden_size_1, batch_size_1 = hyperparameters_1\n",
    "        input_window_size_2, num_of_features_2, hidden_size_2, batch_size_2 = hyperparameters_2\n",
    "\n",
    "        # Load England data\n",
    "        processed_ili_data_england_1, min_ili_england_1, max_ili_england_1 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2017-08-31', training_start_date='2008-09-01')\n",
    "        training_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_1]\n",
    "        testing_query_data_england_1 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_1]\n",
    "        training_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_1]\n",
    "        testing_ili_data_england_1 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_1]\n",
    "        training_ili_data_us_1 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_1]\n",
    "        training_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_1]\n",
    "        testing_query_data_us_1 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_1]\n",
    "\n",
    "        qids_correlated_us_1 = filter_correlated_queries(training_query_data_us_1, training_ili_data_us_1, num_of_features=num_of_features_1)[1]\n",
    "\n",
    "        mapping_us_to_england_1 = query_mapping_variable(qids_correlated_us_1, query_ids_subset, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight, least_k=least_k)\n",
    "\n",
    "        X_test_mapped_1, Y_test_mapped_1, X_train_mapped_1, Y_train_mapped_1, X_test_1, X_train_1, source_X_test_1, qmax_1, Y_train_1, Y_test_1 = prepare_data_with_mapping(testing_query_data_us_1, testing_query_data_england_1, testing_ili_data_england_1, training_query_data_england_1, training_query_data_us_1, training_ili_data_us_1, mapping_us_to_england_1, input_window_size=input_window_size_1, output_window_size=7, qids_correlated=qids_correlated_us_1)\n",
    "\n",
    "        # Load England data\n",
    "        processed_ili_data_england_2, min_ili_england_2, max_ili_england_2 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2018-08-31', training_start_date='2008-09-01')\n",
    "        training_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_2]\n",
    "        testing_query_data_england_2 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_2]\n",
    "        training_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_2]\n",
    "        testing_ili_data_england_2 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_2]\n",
    "        training_ili_data_us_2 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_2]\n",
    "        training_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_2]\n",
    "        testing_query_data_us_2 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_2]\n",
    "\n",
    "        qids_correlated_us_2 = filter_correlated_queries(training_query_data_us_2, training_ili_data_us_2, num_of_features=num_of_features_2)[1]\n",
    "\n",
    "        mapping_us_to_england_2 = query_mapping_variable(qids_correlated_us_2, query_ids_subset, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight, uniform_weight=uniform_weight, least_k=least_k)\n",
    "\n",
    "        X_test_mapped_2, Y_test_mapped_2, X_train_mapped_2, Y_train_mapped_2, X_test_2, X_train_2, source_X_test_2, qmax_2, Y_train_2, Y_test_2 = prepare_data_with_mapping(testing_query_data_us_2, testing_query_data_england_2, testing_ili_data_england_2, training_query_data_england_2, training_query_data_us_2, training_ili_data_us_2, mapping_us_to_england_2, input_window_size=input_window_size_2, output_window_size=7, qids_correlated=qids_correlated_us_2)\n",
    "\n",
    "        corr1, mae1 = plot_predictions_scaled(model_us_1, X_test_mapped_1, Y_test_mapped_1, '2017-09-01', '2018-08-31', output_size=7, min_ili=min_ili_1, max_ili=max_ili_1, min_ili_target=min_ili_england_1, max_ili_target=max_ili_england_1, y_definition='Number of ILI cases per 100,000 population (England)', verbal=False)\n",
    "        corr2, mae2 = plot_predictions_scaled(model_us_2, X_test_mapped_2, Y_test_mapped_2, '2018-09-01', '2019-08-31', output_size=7, min_ili=min_ili_2, max_ili=max_ili_2, min_ili_target=min_ili_england_2, max_ili_target=max_ili_england_2, y_definition='Number of ILI cases per 100,000 population (England)', verbal=False)\n",
    "\n",
    "        corr_season1.append(corr1)\n",
    "        corr_season2.append(corr2)\n",
    "        mae_season1.append(mae1)\n",
    "        mae_season2.append(mae2)\n",
    "\n",
    "    return corr_season1, corr_season2, mae_season1, mae_season2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "\n",
    "for k in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    for semantic_weight in [0, 0.2, 1.0]:\n",
    "        corr_season1, corr_season2, mae_season1, mae_season2 = test_transfer_parameter(k_value=k, semantic_weight=semantic_weight, uniform_weight=False, candidate=500)\n",
    "        performance[(k, semantic_weight)] = np.mean(corr_season1), np.mean(corr_season2), np.mean(mae_season1), np.mean(mae_season2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    print(f''' \\multirow{{3}}{{1em}}{{{k}}} & 0.0 & {performance[(k, 0.0)][0]:.3f} & {performance[(k, 0.0)][2]:.3f} & {performance[(k, 0.0)][1]:.3f} & {performance[(k, 0.0)][3]:.3f} & {(performance[(k, 0.0)][0] + performance[(k, 0.0)][1]) / 2:.3f} & {(performance[(k, 0.0)][2] + performance[(k, 0.0)][3]) / 2:.3f} \\\\\\\\ \n",
    " & 0.2 & {performance[(k, 0.2)][0]:.3f} & {performance[(k, 0.2)][2]:.3f} & {performance[(k, 0.2)][1]:.3f} & {performance[(k, 0.2)][3]:.3f} & {(performance[(k, 0.2)][0] + performance[(k, 0.2)][1]) / 2:.3f} & {(performance[(k, 0.2)][2] + performance[(k, 0.2)][3]) / 2:.3f} \\\\\\\\ \n",
    " & 1.0 & {performance[(k, 1.0)][0]:.3f} & {performance[(k, 1.0)][2]:.3f} & {performance[(k, 1.0)][1]:.3f} & {performance[(k, 1.0)][3]:.3f} & {(performance[(k, 1.0)][0] + performance[(k, 1.0)][1]) / 2:.3f} & {(performance[(k, 1.0)][2] + performance[(k, 1.0)][3]) / 2:.3f} \\\\\\\\ \n",
    " \\\\hline''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for semantic_weight in [0, 0.2, 1.0]:\n",
    "    print(f\"semantic_weight: {semantic_weight}\")\n",
    "    corr_season1, corr_season2, mae_season1, mae_season2 = test_transfer_parameter_variable(semantic_weight=semantic_weight, uniform_weight=False, candidate=500, least_k=3)\n",
    "    results[semantic_weight] = np.mean(corr_season1), np.mean(corr_season2), np.mean(mae_season1), np.mean(mae_season2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f''' \\multirow{{3}}{{1em}}{{Variable}} & 0.0 & {results[0.0][0]:.3f} & {results[0.0][2]:.3f} & {results[0.0][1]:.3f} & {results[0.0][3]:.3f} & {(results[0.0][0] + results[0.0][1]) / 2:.3f} & {(results[0.0][2] + results[0.0][3]) / 2:.3f} \\\\\\\\ \n",
    " & 0.2 & {results[0.2][0]:.3f} & {results[0.2][2]:.3f} & {results[0.2][1]:.3f} & {results[0.2][3]:.3f} & {(results[0.2][0] + results[0.2][1]) / 2:.3f} & {(results[0.2][2] + results[0.2][3]) / 2:.3f} \\\\\\\\ \n",
    " & 1.0 & {results[1.0][0]:.3f} & {results[1.0][2]:.3f} & {results[1.0][1]:.3f} & {results[1.0][3]:.3f} & {(results[1.0][0] + results[1.0][1]) / 2:.3f} & {(results[1.0][2] + results[1.0][3]) / 2:.3f} \\\\\\\\ \n",
    " \\\\hline''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and output results into a file\n",
    "\n",
    "with open('results/results_transfer_variable.txt', 'w') as f:\n",
    "    for key, value in results.items():\n",
    "        f.write(f\"{key}: {value[0]*100:.2f}% {value[1]:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/results_transfer_with_cold.txt', 'a') as f:\n",
    "    for key, value in results.items():\n",
    "        f.write(f\"{key[0]} {key[1]}: {value[0]*100:.2f}% {value[1]:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train_1.cpu().mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [m[0] for m in mapping_us_to_england_1[5276]]\n",
    "\n",
    "[query_ids_dict[q] for q in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_us_to_england_1[2040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to investigate the spike in early curve for X_test_1_all\n",
    "X_test_1_all = pd.concat(X_test_1)\n",
    "\n",
    "# Filter days. Only the first 50 days\n",
    "X_test_1_all_part = X_test_1_all.iloc[10:110]\n",
    "\n",
    "# Which query caused the spike?\n",
    "# Now find the idx of maximum for each query\n",
    "\n",
    "max_idx = X_test_1_all_part.idxmax()\n",
    "\n",
    "# Find the 10 earliest maximum of all queries\n",
    "max_idx.sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single query\n",
    "\n",
    "plt.plot(q_freq_england_smoothed[2040][day_index('2017-09-01'):day_index('2018-08-31')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids_dict[2671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Plot X_test_1 and X_test_2 across time (all queries averaged)\n",
    "X_test_1_all = pd.concat(X_test_1)\n",
    "X_test_2_all = pd.concat(X_test_2)\n",
    "\n",
    "plt.plot(X_test_1_all.mean(axis=1), label='X_test_1')\n",
    "plt.plot(X_test_2_all.mean(axis=1), label='X_test_2')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X_test_1_all.max(axis=1), label='X_test_1')\n",
    "plt.plot(X_test_2_all.max(axis=1), label='X_test_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(X_test_1_all[X_test_1_all.columns[6]], label='X_test_1')\n",
    "plt.plot(X_test_2_all[X_test_2_all.columns[6]], label='X_test_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_random_seed = 55\n",
    "\n",
    "test_seasons_3 = all_seasons[-3:-2]\n",
    "train_seasons_3 = all_seasons[:-3]\n",
    "training_query_data_us_3 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in train_seasons_3]\n",
    "testing_query_data_us_3 = [query_data_for_season(start_date, end_date, q_freq_us_smoothed) for start_date, end_date in test_seasons_3]\n",
    "processed_ili_data_us_3, min_ili_3, max_ili_3 = preprocess_ili_data(ili_raw_us, training_end_date='2016-08-31')\n",
    "training_ili_data_us_3 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in train_seasons_3]\n",
    "testing_ili_data_us_3 = [ili_data_for_season(start_date, end_date, processed_ili_data) for start_date, end_date in test_seasons_3]\n",
    "\n",
    "# Hyperparameters\n",
    "input_window_sizes = [7, 14]\n",
    "nums_of_features = [60, 100]\n",
    "hidden_sizes = [128, 256]\n",
    "batch_sizes = [7, 14, 28]\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_hyperparameters_3 = None\n",
    "best_val_loss_3 = math.inf\n",
    "for input_window_size in input_window_sizes:\n",
    "    for num_of_features in nums_of_features:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(\"--------------------\")\n",
    "                print(\"Hyperparameters(Input window size, number of features, hidden size, batch size):\", input_window_size, num_of_features, hidden_size, batch_size)\n",
    "                model, val_loss = train_nn_with_hyperparameters_transfer(training_query_data_us_3, testing_query_data_us_3, training_ili_data_us_3, testing_ili_data_us_3, input_window_size=input_window_size, num_of_features=num_of_features, batch_size=batch_size, hidden_size=hidden_size, random_seed=universal_random_seed, plot_valid_graph=False)\n",
    "                print(\"Validation loss:\", val_loss)\n",
    "                if val_loss < best_val_loss_3:\n",
    "                    best_val_loss_3 = val_loss\n",
    "                    best_hyperparameters_3 = (input_window_size, num_of_features, hidden_size, batch_size)\n",
    "\n",
    "# Train over entire training data with best hyperparameters\n",
    "input_window_size_3, num_of_features_3, hidden_size_3, batch_size_3 = best_hyperparameters_3\n",
    "print(\"Best hyperparameters:\", best_hyperparameters_3)\n",
    "model_us_3, val_loss_3 = train_nn_with_hyperparameters_transfer(training_query_data_us_3, testing_query_data_us_3, training_ili_data_us_3, testing_ili_data_us_3, input_window_size=input_window_size_3, num_of_features=num_of_features_3, batch_size=batch_size_3, hidden_size=hidden_size_3, random_seed=universal_random_seed)\n",
    "\n",
    "train_loader_3, X_train_3, Y_train_3, X_test_3, Y_test_3 = prepare_data_without_validation(training_query_data_us_3, testing_query_data_us_3, training_ili_data_us_3, testing_ili_data_us_3, input_window_size=input_window_size_3, output_window_size=7, num_of_features=num_of_features_3, batch_size=batch_size_3)\n",
    "\n",
    "plot_predictions(model_us_3, X_test_3, Y_test_3, '2016-09-01', '2017-08-31', output_size=7, min_ili=min_ili_3, max_ili=max_ili_3, y_definition='Number of ILI cases per 100,000 population (US)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids_subset = set(list(qids_all_sorted)[:1000])\n",
    "k_value = 5\n",
    "semantic_weight = 0.2\n",
    "\n",
    "# Load England data\n",
    "processed_ili_data_england_3, min_ili_england_3, max_ili_england_3 = preprocess_ili_data(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2016-08-31', training_start_date='2008-09-01')\n",
    "training_query_data_england_3 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in train_seasons_3]\n",
    "testing_query_data_england_3 = [query_data_for_season(start_date, end_date, q_freq_england_smoothed) for start_date, end_date in test_seasons_3]\n",
    "training_ili_data_england_3 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in train_seasons_3]\n",
    "testing_ili_data_england_3 = [ili_data_for_season(start_date, end_date, processed_ili_data_england) for start_date, end_date in test_seasons_3]\n",
    "\n",
    "qids_correlated_us_3 = filter_correlated_queries(training_query_data_us_3, training_ili_data_us_3, num_of_features=num_of_features_3)[1]\n",
    "\n",
    "mapping_us_to_england_3 = query_mapping(qids_correlated_us_3, query_ids_subset, k=k_value, source_query_data=q_freq_us_smoothed, target_query_data=q_freq_england_smoothed, semantic_weight=semantic_weight)\n",
    "\n",
    "X_test_mapped_3, Y_test_mapped_3, X_train_mapped_3, Y_train_mapped_3, X_test_3, X_train_3, source_X_test_3, qmax_3, Y_train_3, Y_test_3 = prepare_data_with_mapping(testing_query_data_us_3, testing_query_data_england_3, testing_ili_data_england_3, training_query_data_england_3, training_query_data_us_3, training_ili_data_us_3, mapping_us_to_england_3, input_window_size=input_window_size_3, output_window_size=7, qids_correlated=qids_correlated_us_3)\n",
    "\n",
    "plot_predictions_scaled(model_us_3, X_test_mapped_3, Y_test_mapped_3, '2016-09-01', '2017-08-31', output_size=7, min_ili=min_ili_3, max_ili=max_ili_3, min_ili_target=min_ili_england_3, max_ili_target=max_ili_england_3, y_definition='Number of ILI cases per 100,000 population (England)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "X_test_3_all = pd.concat(X_test_3)\n",
    "\n",
    "# plt.plot(X_test_3_all.max(axis=1), label='X_test_3')\n",
    "# plt.plot(X_test_3_all.mean(axis=1), label='X_test_3')\n",
    "for i in range(60):\n",
    "    plt.plot(X_test_3_all[X_test_3_all.columns[i]], label='X_test_3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test_1_all.mean(axis=1), label='X_test_2')\n",
    "for i in range(100):\n",
    "    plt.plot(X_test_1_all[X_test_1_all.columns[i]], label='X_test_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ili_data_without_normalization(ili_raw, time_format=\"%Y-%m-%d\", training_end_date='2017-08-31', training_start_date='2008-09-01'):\n",
    "    ili_raw['Day'] = ili_raw['WeekStart'].apply(day_index, time_format=time_format)\n",
    "    ili_raw = ili_raw.set_index('Day')\n",
    "    ili_raw = ili_raw[~ili_raw.index.duplicated(keep='first')]\n",
    "    ili_raw = ili_raw.reindex(range(ili_raw.index.min(), ili_raw.index.max() + 1), fill_value=None)\n",
    "    ili_raw = ili_raw.reset_index()\n",
    "\n",
    "    ili_raw = ili_raw[['Day', 'ILIRate']]\n",
    "    ili_raw = ili_raw[ili_raw['Day'].between(dataset_start_day, dataset_end_day)]\n",
    "\n",
    "    # Interpolate missing values with linear interpolation\n",
    "    ili_raw['ILIRate'] = ili_raw['ILIRate'].interpolate(method='linear')\n",
    "\n",
    "    # Extrapolate so starting days and ending days have values\n",
    "    ili_raw['ILIRate'] = ili_raw['ILIRate'].bfill()\n",
    "\n",
    "    ili_raw_training = ili_raw[ili_raw['Day'].between(day_index(training_start_date), day_index(training_end_date))]\n",
    "    \n",
    "    return ili_raw['ILIRate'], min_ili, max_ili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ili_us_raw_without_normalization, min_ili_us, max_ili_us = preprocess_ili_data_without_normalization(ili_raw_us, training_start_date='2008-09-01', training_end_date='2017-08-31')\n",
    "ili_england_raw_without_normalization, min_ili_england, max_ili_england = preprocess_ili_data_without_normalization(ili_raw_england, time_format=\"%m/%d/%Y\", training_end_date='2017-08-31', training_start_date='2008-09-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw both England and US ILI rates on the same plot, without normalization\n",
    "\n",
    "plt.plot(list(ili_us_raw_without_normalization[730+365:]), label='US')\n",
    "plt.plot(list(ili_england_raw_without_normalization[730+365:]), label='England')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(np.arange(0, len(ili_us_raw_without_normalization[730+365:]), 365), np.arange(2008+3, 2020), rotation=45)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"ILI Rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
